"""
ðŸ”„ Data Sync Agent - Database & Storage Synchronization System
Multi-platform data synchronization and management

Made with â¤ï¸ by Mulky Malikul Dhaher in Indonesia ðŸ‡®ðŸ‡©
"""

import asyncio
import json
import os
import time
import sqlite3
import redis
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from pathlib import Path
import uuid
import hashlib
import aiofiles
import asyncpg

from colony.core.base_agent import BaseAgent
from colony.core.agent_registry import register_agent

@register_agent(name="data_sync_agent")
class DataSyncAgent(BaseAgent):
    """
    Data Synchronization Agent that:
    - Syncs data across multiple databases (SQLite, PostgreSQL, Redis)
    - Manages Supabase real-time synchronization
    - Handles JSON file storage and backup
    - Provides data validation and integrity checks
    - Manages data migrations and transformations
    - Implements conflict resolution strategies
    """

    def __init__(self, name: str, config: Dict[str, Any], memory_manager: Any):
        super().__init__(name, config, memory_manager)
        self.connections = {}
        self.sync_configs = self._load_sync_configs()
        
        # Sync tracking
        self.sync_operations: Dict[str, Dict] = {}
        self.last_sync_times: Dict[str, datetime] = {}
        
        # Initialize connections
        self._initialize_connections()

    def run(self, **kwargs):
        """The main entry point for the agent's execution."""
        self.update_status("running")
        # This agent is designed to be called with specific tasks,
        # so the run method will just keep the agent alive.
        while self.status == "running":
            time.sleep(1)

    def _load_sync_configs(self) -> Dict[str, Dict]:
        """Load synchronization configurations"""
        return {
            "sqlite_local": {
                "type": "sqlite",
                "path": "data/agentic.db",
                "priority": 1,
                "sync_direction": "bidirectional"
            },
            "supabase": {
                "type": "postgresql",
                "host": os.getenv("SUPABASE_URL", "").replace("https://", "").replace(".supabase.co", ""),
                "database": "postgres",
                "user": "postgres",
                "password": os.getenv("SUPABASE_SERVICE_ROLE_KEY", ""),
                "priority": 2,
                "sync_direction": "bidirectional",
                "real_time": True
            },
            "redis_cache": {
                "type": "redis",
                "host": os.getenv("REDIS_HOST", "localhost"),
                "port": int(os.getenv("REDIS_PORT", "6379")),
                "priority": 3,
                "sync_direction": "push",
                "ttl": 3600
            },
            "json_backup": {
                "type": "json",
                "path": "data/backups",
                "priority": 4,
                "sync_direction": "push",
                "schedule": "hourly"
            }
        }
    
    def _initialize_connections(self):
        """Initialize database connections"""
        # SQLite connection
        try:
            sqlite_path = self.sync_configs["sqlite_local"]["path"]
            Path(sqlite_path).parent.mkdir(parents=True, exist_ok=True)
            self.connections["sqlite"] = sqlite3.connect(sqlite_path, check_same_thread=False)
            self._setup_sqlite_tables()
            print("âœ… SQLite connection established")
        except Exception as e:
            print(f"âš ï¸ SQLite connection failed: {e}")
        
        # Redis connection
        try:
            redis_config = self.sync_configs["redis_cache"]
            self.connections["redis"] = redis.Redis(
                host=redis_config["host"],
                port=redis_config["port"],
                decode_responses=True
            )
            self.connections["redis"].ping()
            print("âœ… Redis connection established")
        except Exception as e:
            print(f"âš ï¸ Redis connection failed: {e}")
    
    def _setup_sqlite_tables(self):
        """Setup SQLite tables for system data"""
        conn = self.connections.get("sqlite")
        if not conn:
            return
        
        cursor = conn.cursor()
        
        # Sync operations table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS sync_operations (
                id TEXT PRIMARY KEY,
                operation_type TEXT NOT NULL,
                source_db TEXT NOT NULL,
                target_db TEXT NOT NULL,
                table_name TEXT NOT NULL,
                record_count INTEGER DEFAULT 0,
                status TEXT DEFAULT 'pending',
                started_at DATETIME,
                completed_at DATETIME,
                error_message TEXT
            )
        """)
        
        # Data conflicts table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS data_conflicts (
                id TEXT PRIMARY KEY,
                table_name TEXT NOT NULL,
                record_id TEXT NOT NULL,
                source_data TEXT NOT NULL,
                target_data TEXT NOT NULL,
                conflict_type TEXT NOT NULL,
                resolved BOOLEAN DEFAULT FALSE,
                resolution_strategy TEXT,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        # Agent data table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS agent_data (
                id TEXT PRIMARY KEY,
                agent_id TEXT NOT NULL,
                data_type TEXT NOT NULL,
                data_content TEXT NOT NULL,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                version INTEGER DEFAULT 1
            )
        """)
        
        # System metrics table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS system_metrics (
                id TEXT PRIMARY KEY,
                metric_name TEXT NOT NULL,
                metric_value REAL NOT NULL,
                metric_unit TEXT,
                agent_id TEXT,
                recorded_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        conn.commit()
    
    async def process_task(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """Process data synchronization task"""
        try:
            action = task.get("action", "sync_all")
            
            if action == "sync_all":
                return await self._sync_all_databases()
            elif action == "sync_specific":
                return await self._sync_specific_table(task)
            elif action == "backup_data":
                return await self._backup_data(task)
            elif action == "restore_data":
                return await self._restore_data(task)
            elif action == "resolve_conflicts":
                return await self._resolve_conflicts(task)
            elif action == "cleanup_memory":
                return await self._cleanup_old_data()
            elif action == "validate_integrity":
                return await self._validate_data_integrity()
            elif action == "supabase_sync":
                return await self._sync_supabase()
            else:
                return self._create_error_response(f"Unknown action: {action}")
                
        except Exception as e:
            return self._create_error_response(str(e))
    
    async def _sync_all_databases(self) -> Dict[str, Any]:
        """Synchronize all configured databases"""
        sync_id = str(uuid.uuid4())
        
        try:
            results = {}
            total_synced = 0
            
            # Memory to SQLite sync
            if self.memory:
                memory_result = await self._sync_memory_to_sqlite()
                results["memory_to_sqlite"] = memory_result
                total_synced += memory_result.get("synced_records", 0)
            
            # SQLite to Supabase sync
            supabase_result = await self._sync_sqlite_to_supabase()
            results["sqlite_to_supabase"] = supabase_result
            total_synced += supabase_result.get("synced_records", 0)
            
            # Cache popular data to Redis
            redis_result = await self._cache_to_redis()
            results["cache_to_redis"] = redis_result
            total_synced += redis_result.get("cached_records", 0)
            
            # Create JSON backup
            backup_result = await self._create_json_backup()
            results["json_backup"] = backup_result
            
            # Record sync operation
            self._record_sync_operation(sync_id, "full_sync", "all", "all", total_synced, "completed")
            
            return {
                "success": True,
                "sync_id": sync_id,
                "total_synced": total_synced,
                "results": results,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self._record_sync_operation(sync_id, "full_sync", "all", "all", 0, "failed", str(e))
            return self._create_error_response(f"Sync failed: {str(e)}")
    
    async def _sync_memory_to_sqlite(self) -> Dict[str, Any]:
        """Sync memory bus data to SQLite"""
        if not self.memory or "sqlite" not in self.connections:
            return {"success": False, "error": "Memory bus or SQLite not available"}
        
        try:
            # Get recent memory entries
            recent_tasks = self.memory.get_recent_tasks(limit=100)
            agent_metrics = {}  # Would get from memory if available
            
            conn = self.connections["sqlite"]
            cursor = conn.cursor()
            
            synced_count = 0
            
            # Sync tasks
            for task in recent_tasks:
                cursor.execute("""
                    INSERT OR REPLACE INTO agent_data 
                    (id, agent_id, data_type, data_content, updated_at)
                    VALUES (?, ?, ?, ?, ?)
                """, (
                    task.get("task_id", str(uuid.uuid4())),
                    task.get("assigned_agent", "unknown"),
                    "task",
                    json.dumps(task),
                    datetime.now()
                ))
                synced_count += 1
            
            conn.commit()
            
            return {
                "success": True,
                "synced_records": synced_count,
                "source": "memory_bus",
                "target": "sqlite"
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def _sync_sqlite_to_supabase(self) -> Dict[str, Any]:
        """Sync SQLite data to Supabase"""
        try:
            # Check if Supabase is configured
            supabase_config = self.sync_configs.get("supabase")
            if not supabase_config or not supabase_config.get("password"):
                return {"success": False, "error": "Supabase not configured"}
            
            # For now, return mock sync (would implement real Supabase sync)
            return {
                "success": True,
                "synced_records": 0,
                "source": "sqlite",
                "target": "supabase",
                "note": "Supabase sync configured but not implemented in this demo"
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def _cache_to_redis(self) -> Dict[str, Any]:
        """Cache frequently accessed data to Redis"""
        if "redis" not in self.connections:
            return {"success": False, "error": "Redis not available"}
        
        try:
            redis_conn = self.connections["redis"]
            cached_count = 0
            
            # Cache agent status data
            if "sqlite" in self.connections:
                conn = self.connections["sqlite"]
                cursor = conn.cursor()
                
                # Cache recent agent data
                cursor.execute("""
                    SELECT agent_id, data_type, data_content, updated_at
                    FROM agent_data
                    WHERE updated_at > datetime('now', '-1 hour')
                    ORDER BY updated_at DESC
                    LIMIT 50
                """)
                
                for row in cursor.fetchall():
                    cache_key = f"agent:{row[0]}:{row[1]}"
                    cache_data = {
                        "content": json.loads(row[2]),
                        "updated_at": row[3]
                    }
                    
                    redis_conn.setex(
                        cache_key,
                        3600,  # 1 hour TTL
                        json.dumps(cache_data)
                    )
                    cached_count += 1
            
            # Cache system status
            system_status = {
                "agents_active": len(self.sync_configs),
                "last_sync": datetime.now().isoformat(),
                "sync_agent_status": "active"
            }
            
            redis_conn.setex(
                "system:status",
                300,  # 5 minutes TTL
                json.dumps(system_status)
            )
            cached_count += 1
            
            return {
                "success": True,
                "cached_records": cached_count,
                "target": "redis"
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def _create_json_backup(self) -> Dict[str, Any]:
        """Create JSON backup of critical data"""
        try:
            backup_dir = Path("data/backups")
            backup_dir.mkdir(parents=True, exist_ok=True)
            
            backup_file = backup_dir / f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            backup_data = {
                "created_at": datetime.now().isoformat(),
                "backup_type": "automated",
                "agent_id": self.name,
                "data": {}
            }
            
            # Backup SQLite data
            if "sqlite" in self.connections:
                conn = self.connections["sqlite"]
                cursor = conn.cursor()
                
                # Backup agent data
                cursor.execute("SELECT * FROM agent_data ORDER BY updated_at DESC LIMIT 100")
                agent_data = []
                for row in cursor.fetchall():
                    agent_data.append({
                        "id": row[0],
                        "agent_id": row[1],
                        "data_type": row[2],
                        "data_content": json.loads(row[3]),
                        "created_at": row[4],
                        "updated_at": row[5],
                        "version": row[6]
                    })
                
                backup_data["data"]["agent_data"] = agent_data
                
                # Backup system metrics
                cursor.execute("SELECT * FROM system_metrics ORDER BY recorded_at DESC LIMIT 50")
                metrics_data = []
                for row in cursor.fetchall():
                    metrics_data.append({
                        "id": row[0],
                        "metric_name": row[1],
                        "metric_value": row[2],
                        "metric_unit": row[3],
                        "agent_id": row[4],
                        "recorded_at": row[5]
                    })
                
                backup_data["data"]["system_metrics"] = metrics_data
            
            # Save backup file
            async with aiofiles.open(backup_file, 'w') as f:
                await f.write(json.dumps(backup_data, indent=2))
            
            # Clean up old backups (keep last 10)
            backups = sorted(backup_dir.glob("backup_*.json"))
            if len(backups) > 10:
                for old_backup in backups[:-10]:
                    old_backup.unlink()
            
            return {
                "success": True,
                "backup_file": str(backup_file),
                "backup_size": backup_file.stat().st_size,
                "records_backed_up": len(backup_data["data"].get("agent_data", [])) + len(backup_data["data"].get("system_metrics", []))
            }
            
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    async def _cleanup_old_data(self) -> Dict[str, Any]:
        """Clean up old data to free space"""
        try:
            cleaned_records = 0
            
            if "sqlite" in self.connections:
                conn = self.connections["sqlite"]
                cursor = conn.cursor()
                
                # Clean up old sync operations (older than 30 days)
                cursor.execute("""
                    DELETE FROM sync_operations 
                    WHERE started_at < datetime('now', '-30 days')
                """)
                cleaned_records += cursor.rowcount
                
                # Clean up resolved conflicts (older than 7 days)
                cursor.execute("""
                    DELETE FROM data_conflicts 
                    WHERE resolved = TRUE AND created_at < datetime('now', '-7 days')
                """)
                cleaned_records += cursor.rowcount
                
                # Clean up old agent data (keep last 1000 records per agent)
                cursor.execute("""
                    DELETE FROM agent_data 
                    WHERE id NOT IN (
                        SELECT id FROM agent_data 
                        ORDER BY updated_at DESC 
                        LIMIT 1000
                    )
                """)
                cleaned_records += cursor.rowcount
                
                # Clean up old metrics (older than 7 days)
                cursor.execute("""
                    DELETE FROM system_metrics 
                    WHERE recorded_at < datetime('now', '-7 days')
                """)
                cleaned_records += cursor.rowcount
                
                conn.commit()
            
            # Clean up Redis expired keys
            if "redis" in self.connections:
                # Redis handles TTL automatically, but we can clean up manually
                redis_conn = self.connections["redis"]
                
                # Remove old agent status keys
                pattern_keys = redis_conn.keys("agent:*")
                for key in pattern_keys:
                    try:
                        data = json.loads(redis_conn.get(key) or "{}")
                        if "updated_at" in data:
                            updated_time = datetime.fromisoformat(data["updated_at"])
                            if datetime.now() - updated_time > timedelta(hours=24):
                                redis_conn.delete(key)
                                cleaned_records += 1
                    except:
                        continue
            
            return {
                "success": True,
                "cleaned_records": cleaned_records,
                "cleanup_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            return self._create_error_response(f"Cleanup failed: {str(e)}")
    
    async def _validate_data_integrity(self) -> Dict[str, Any]:
        """Validate data integrity across systems"""
        try:
            validation_results = {
                "sqlite": {"status": "unknown", "issues": []},
                "redis": {"status": "unknown", "issues": []},
                "json_backups": {"status": "unknown", "issues": []}
            }
            
            # Validate SQLite
            if "sqlite" in self.connections:
                conn = self.connections["sqlite"]
                cursor = conn.cursor()
                
                try:
                    # Check for data integrity
                    cursor.execute("PRAGMA integrity_check")
                    result = cursor.fetchone()
                    
                    if result and result[0] == "ok":
                        validation_results["sqlite"]["status"] = "healthy"
                    else:
                        validation_results["sqlite"]["status"] = "issues"
                        validation_results["sqlite"]["issues"].append("Integrity check failed")
                    
                    # Check for orphaned records
                    cursor.execute("SELECT COUNT(*) FROM agent_data WHERE agent_id = ''")
                    orphaned_count = cursor.fetchone()[0]
                    
                    if orphaned_count > 0:
                        validation_results["sqlite"]["issues"].append(f"{orphaned_count} orphaned records")
                    
                except Exception as e:
                    validation_results["sqlite"]["status"] = "error"
                    validation_results["sqlite"]["issues"].append(str(e))
            
            # Validate Redis
            if "redis" in self.connections:
                try:
                    redis_conn = self.connections["redis"]
                    redis_conn.ping()
                    
                    # Check memory usage
                    info = redis_conn.info("memory")
                    used_memory = info.get("used_memory", 0)
                    max_memory = info.get("maxmemory", 0)
                    
                    if max_memory > 0 and used_memory / max_memory > 0.9:
                        validation_results["redis"]["issues"].append("High memory usage")
                    
                    validation_results["redis"]["status"] = "healthy"
                    
                except Exception as e:
                    validation_results["redis"]["status"] = "error"
                    validation_results["redis"]["issues"].append(str(e))
            
            # Validate JSON backups
            try:
                backup_dir = Path("data/backups")
                if backup_dir.exists():
                    backups = list(backup_dir.glob("backup_*.json"))
                    
                    if len(backups) == 0:
                        validation_results["json_backups"]["issues"].append("No backups found")
                    else:
                        # Check latest backup
                        latest_backup = max(backups, key=lambda p: p.stat().st_mtime)
                        backup_age = time.time() - latest_backup.stat().st_mtime
                        
                        if backup_age > 86400:  # 24 hours
                            validation_results["json_backups"]["issues"].append("Latest backup is old")
                    
                    validation_results["json_backups"]["status"] = "healthy"
                else:
                    validation_results["json_backups"]["issues"].append("Backup directory not found")
                    
            except Exception as e:
                validation_results["json_backups"]["status"] = "error"
                validation_results["json_backups"]["issues"].append(str(e))
            
            # Overall health status
            overall_status = "healthy"
            total_issues = sum(len(result["issues"]) for result in validation_results.values())
            
            if total_issues > 0:
                overall_status = "issues_found"
            
            if any(result["status"] == "error" for result in validation_results.values()):
                overall_status = "errors"
            
            return {
                "success": True,
                "overall_status": overall_status,
                "total_issues": total_issues,
                "validation_results": validation_results,
                "validated_at": datetime.now().isoformat()
            }
            
        except Exception as e:
            return self._create_error_response(f"Validation failed: {str(e)}")
    
    def _record_sync_operation(self, sync_id: str, operation_type: str, source: str, 
                             target: str, record_count: int, status: str, error: str = None):
        """Record sync operation in database"""
        try:
            if "sqlite" not in self.connections:
                return
            
            conn = self.connections["sqlite"]
            cursor = conn.cursor()
            
            cursor.execute("""
                INSERT OR REPLACE INTO sync_operations
                (id, operation_type, source_db, target_db, table_name, record_count, 
                 status, started_at, completed_at, error_message)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                sync_id,
                operation_type,
                source,
                target,
                "multiple",
                record_count,
                status,
                datetime.now(),
                datetime.now() if status in ["completed", "failed"] else None,
                error
            ))
            
            conn.commit()
            
        except Exception as e:
            print(f"Failed to record sync operation: {e}")
    
    def get_sync_status(self) -> Dict[str, Any]:
        """Get current synchronization status"""
        status = {
            "agent_id": self.name,
            "status": self.status,
            "active_connections": len(self.connections),
            "configured_databases": len(self.sync_configs),
            "last_sync_times": {k: v.isoformat() for k, v in self.last_sync_times.items()},
            "active_operations": len(self.sync_operations)
        }
        
        # Add connection status
        for conn_name, conn in self.connections.items():
            try:
                if conn_name == "sqlite":
                    conn.execute("SELECT 1")
                    status[f"{conn_name}_status"] = "connected"
                elif conn_name == "redis":
                    conn.ping()
                    status[f"{conn_name}_status"] = "connected"
            except:
                status[f"{conn_name}_status"] = "disconnected"
        
        return status
    
    def _create_error_response(self, error_message: str) -> Dict[str, Any]:
        """Create standardized error response"""
        return {
            "success": False,
            "error": error_message,
            "agent": self.name,
            "timestamp": datetime.now().isoformat()
        }
